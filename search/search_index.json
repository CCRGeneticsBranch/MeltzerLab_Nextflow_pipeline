{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Background","text":""},{"location":"#meltzerlab-data-processing-pipeline","title":"Meltzerlab Data Processing Pipeline","text":"<p>This repository contains a Nextflow based DNA/RNA data processing pipeline built specifically for Meltzer lab. This pipeline is built to run variant filtering and extensive QC on a sequencing run. The pipeline supports processing data from multiple references. We currently support</p> Genome Version Human hg19, hg38 Mouse mm10, mm39 Dog canFam3, canFam6"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<p>This pipeline is developed and deployed solely on NIH Biowulf.</p>"},{"location":"#biowulf","title":"Biowulf","text":"<p>This pipeline is hosted under /data/GBNCI directory. You'll need to start an interactive session inorder to launch the pipeline.</p> <p><code>sinteractive --mem=30g --cpus-per-task=4</code></p>"},{"location":"#usage","title":"Usage","text":"<pre><code>Usage: /data/GBNCI/MeltzerLab_Nextflow_pipeline/nf.sh  &lt;samplesheet&gt; \n\nThis script requires one positional argument:\n&lt;samplesheet&gt;: Provide the full path to the samplesheet in a Meltzer lab accepted format.\neg: /data/GBNCI/MeltzerLab_Nextflow_pipeline/nf.sh /data/GBNCI/DATA/VG_test/samplesheet.tsv \n</code></pre>"},{"location":"#input-samplesheet","title":"Input Samplesheet","text":"<p>Input file is a tab seperated samplesheet file from Meltzer lab. However the pipeline can be launched as long as these columns are present. Here is an example of the expected format.</p> flowcell library sample_type genome read1 read2 capture_targets AAZ123 L080123_RNA Total RNA hg19 L080123_RNA_R1.fastq.gz L080123_RNA_R1.fastq.gz APC_TP53_BAC_KlenowBait"},{"location":"#help-contributing","title":"Help &amp; Contributing","text":"<p>Come across a bug? Open an issue and include a minimal reproducible example.</p> <p>Have a question? Ask it in discussions.</p> <p>Want to contribute to this project? Check out the contributing guidelines.</p>"},{"location":"#references","title":"References","text":"<p>This repo was originally generated from the CCBR Nextflow Template. The template takes inspiration from nektool1 and the nf-core template. If you plan to contribute your pipeline to nf-core, don't use this template -- instead follow nf-core's instructions2.</p> <p>Information on who the pipeline was developed for, and a statement if it's only been tested on Biowulf. For example:</p> <p>It has been developed and tested solely on NIH HPC Biowulf.</p> <p>Also include a workflow image to summarize the pipeline.</p> <ol> <li> <p>nektool https://github.com/beardymcjohnface/nektool \u21a9</p> </li> <li> <p>instructions for nf-core pipelines https://nf-co.re/docs/contributing/tutorials/creating_with_nf_core \u21a9</p> </li> </ol>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#tool_name-development-version","title":"TOOL_NAME development version","text":""},{"location":"changelog/#api-breaking-changes","title":"API-breaking changes","text":"<ul> <li>change 1</li> <li>example 2</li> </ul>"},{"location":"changelog/#new-features","title":"New features","text":"<ul> <li>new feat 1</li> </ul>"},{"location":"changelog/#bug-fixes","title":"Bug fixes","text":"<ul> <li>description of bug fixed</li> </ul>"},{"location":"changelog/#tool_name-v010","title":"TOOL_NAME v0.1.0","text":"<p>This is the first release of TOOL_NAME \ud83c\udf89</p>"},{"location":"contributing/","title":"Contributing to TOOL_NAME","text":"<p>TODO -- describe gitflow, require PRs...</p>"},{"location":"contributing/#use-pre-commit-hooks","title":"Use pre-commit hooks","text":"<p>Pre-commit can automatically format your code, check for spelling errors, etc. every time you commit.</p> <p>Install pre-commit if you haven't already, then run <code>pre-commit install</code> to install the hooks specified in <code>.pre-commit-config.yaml</code>. Pre-commit will run the hooks every time you commit.</p>"},{"location":"contributing/#versions","title":"Versions","text":"<p>Increment the version number following semantic versioning1 in the <code>VERSION</code> file.</p>"},{"location":"contributing/#changelog","title":"Changelog","text":"<p>Keep the changelog up to date with all changes in <code>docs/CHANGELOG.md</code>.</p>"},{"location":"contributing/#vs-code-extensions","title":"VS code extensions","text":"<p>If you use VS code, installing nf-core extension pack is recommended.</p> <ol> <li> <p>semantic versioning guidelines https://semver.org/ \u21a9</p> </li> </ol>"},{"location":"contributors/","title":"Contributors","text":"<p>Should include a list of all contributors, including GitHub handles when appropriate. In addition, a statement of who contributed to the source code specifically, identified by initials. An example is included below.</p> <p>TODO: populate this automagically similar to https://nf-co.re/contributors? or link to GitHub contributor page? could use gh action: https://github.com/lowlighter/metrics/blob/master/source/plugins/contributors/README.md</p>"},{"location":"contributors/#contributions","title":"Contributions","text":"<p>The following members contributed to the development of the CARLISLE pipeline:</p> <ul> <li>Samantha Sevilla</li> </ul> <p>SS contributed to the generating the source code and all members contributed to the main concepts and analysis.</p>"},{"location":"user-guide/getting-started/","title":"1. Getting Started","text":"<p>This should set the stage for all of the pipeline requirements. Examples are listed below.</p>"},{"location":"user-guide/getting-started/#overview","title":"Overview","text":"<p>The CARLISLE github repository is stored locally, and will be used for project deployment. Multiple projects can be deployed from this one point simultaneously, without concern.</p>"},{"location":"user-guide/getting-started/#1-getting-started","title":"1. Getting Started","text":""},{"location":"user-guide/getting-started/#11-introduction","title":"1.1 Introduction","text":"<p>The CARLISLE Pipelie beings with raw FASTQ files and performs trimming followed by alignment using BOWTIE2. Data is then normalized through either the use of an user-species species (IE E.Coli) spike-in control or through the determined library size. Peaks are then called using MACS2, SEACR, and GoPEAKS with various options selected by the user. Peaks are then annotated, and summarized into reports. If designated, differential analysis is performed using DESEQ2. QC reports are also generated with each project using FASTQC and MULTIQC. Annotations are added using HOMER and ROSE. GSEA Enrichment analysis predictions are added using CHIPENRICH.</p> <p>The following are sub-commands used within CARLISLE:</p> <ul> <li>initialize: initialize the pipeline</li> <li>dryrun: predict the binding of peptides to any MHC molecule</li> <li>cluster: execute the pipeline on the Biowulf HPC</li> <li>local: execute a local, interactive, session</li> <li>git: execute GitHub actions</li> <li>unlock: unlock directory</li> <li>DAG: create DAG report</li> <li>report: create SNAKEMAKE report</li> <li>testrun: copies test manifests and files to WORKDIR</li> </ul>"},{"location":"user-guide/getting-started/#12-setup-dependencies","title":"1.2 Setup Dependencies","text":"<p>CARLISLE has several dependencies listed below. These dependencies can be installed by a sysadmin. All dependencies will be automatically loaded if running from Biowulf.</p> <ul> <li>bedtools: \"bedtools/2.30.0\"</li> <li>bedops: \"bedops/2.4.40\"</li> </ul>"},{"location":"user-guide/getting-started/#13-login-to-the-cluster","title":"1.3 Login to the cluster","text":"<p>CARLISLE has been exclusively tested on Biowulf HPC. Login to the cluster's head node and move into the pipeline location.</p> <pre><code># ssh into cluster's head node\nssh -Y $USER@biowulf.nih.gov\n</code></pre>"},{"location":"user-guide/getting-started/#14-load-an-interactive-session","title":"1.4 Load an interactive session","text":"<p>An interactive session should be started before performing any of the pipeline sub-commands, even if the pipeline is to be executed on the cluster.</p> <pre><code># Grab an interactive node\nsinteractive --time=12:00:00 --mem=8gb  --cpus-per-task=4 --pty bash\n</code></pre>"},{"location":"user-guide/output/","title":"4. Expected Output","text":"<p>This should include all pertitant information about output files, including extensions that differentiate files. An example is provided below.</p>"},{"location":"user-guide/output/#4-expected-outputs","title":"4. Expected Outputs","text":"<p>The following directories are created under the WORKDIR/results directory:</p> <ul> <li>alignment_stats: this directory include information on the alignment of each sample</li> <li>peaks: this directory contains a sub-directory that relates to the quality threshold used.</li> <li>quality threshold<ul> <li>contrasts: this directory includes the contrasts for each line listed in the contrast manifest</li> <li>peak_caller: this directory includes all peak calls from each peak_caller (SEACR, MACS2, GOPEAKS) for each sample</li> <li>annotation<ul> <li>go_enrichment: this directory includes gene set enrichment pathway predictions</li> <li>homer: this directory includes the annotation output from HOMER</li> <li>rose: this directory includes the annotation output from ROSE</li> </ul> </li> </ul> </li> </ul> <pre><code>\u251c\u2500\u2500 alignment_stats\n\u251c\u2500\u2500 bam\n\u251c\u2500\u2500 peaks\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 0.05\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrasts\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 gopeaks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 annotation\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 go_enrichment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status.go_enrichment_tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status.go_enrichment_html_report\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homer\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.gopeaks_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.gopeaks_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rose\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.gopeaks_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.dedup.gopeaks_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.dedup.gopeaks_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 peak_output\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 macs2\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 annotation\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 go_enrichment\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 contrast_id1.dedup_status.go_enrichment_tables\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 contrast_id2.dedup_status.go_enrichment_html_report\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homer\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_narrow.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_broad.motifs\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 homerResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 knownResults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 rose\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id1_vs_control_id.dedup_status.macs2_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_broad.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 replicate_id2_vs_control_id.dedup_status.macs2_narrow.12500\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 peak_output\n</code></pre>"},{"location":"user-guide/preparing-files/","title":"2. Preparing Files","text":"<p>This should describe any input files needed, including config files, manifest files, and sample files. An example is provided below.</p>"},{"location":"user-guide/preparing-files/#2-preparing-files","title":"2. Preparing Files","text":"<p>The pipeline is controlled through editing configuration and manifest files. Defaults are found in the /WORKDIR/config and /WORKDIR/manifest directories, after initialization.</p>"},{"location":"user-guide/preparing-files/#21-configs","title":"2.1 Configs","text":"<p>The configuration files control parameters and software of the pipeline. These files are listed below:</p> <ul> <li>config/config.yaml</li> <li>resources/cluster.yaml</li> <li>resources/tools.yaml</li> </ul>"},{"location":"user-guide/preparing-files/#211-cluster-config","title":"2.1.1 Cluster Config","text":"<p>The cluster configuration file dictates the resources to be used during submission to Biowulf HPC. There are two different ways to control these parameters - first, to control the default settings, and second, to create or edit individual rules. These parameters should be edited with caution, after significant testing.</p>"},{"location":"user-guide/preparing-files/#212-tools-config","title":"2.1.2 Tools Config","text":"<p>The tools configuration file dictates the version of each software or program that is being used in the pipeline.</p>"},{"location":"user-guide/preparing-files/#213-config-yaml","title":"2.1.3 Config YAML","text":"<p>There are several groups of parameters that are editable for the user to control the various aspects of the pipeline. These are :</p> <ul> <li>Folders and Paths</li> <li>These parameters will include the input and output files of the pipeline, as well as list all manifest names.</li> <li>User parameters</li> <li>These parameters will control the pipeline features. These include thresholds and whether to perform processes.</li> <li>References</li> <li>These parameters will control the location of index files, spike-in references, adaptors and species calling information.</li> </ul>"},{"location":"user-guide/preparing-files/#2131-user-parameters","title":"2.1.3.1 User Parameters","text":""},{"location":"user-guide/preparing-files/#21311-duplication-status","title":"2.1.3.1.1 Duplication Status","text":"<p>Users can select duplicated peaks (dedup) or non-deduplicated peaks (no_dedup) through the user parameter.</p> <pre><code>dupstatus: \"dedup, no_dedup\"\n</code></pre>"},{"location":"user-guide/preparing-files/#21312-macs2-additional-option","title":"2.1.3.1.2 Macs2 additional option","text":"<p>MACS2 can be run with or without the control. adding a control will increase peak specificity Selecting \"Y\" for the <code>macs2_control</code> will run the paired control sample provided in the sample manifest</p>"},{"location":"user-guide/preparing-files/#2132-references","title":"2.1.3.2 References","text":"<p>Additional reference files may be added to the pipeline, if other species were to be used.</p> <p>The absolute file paths which must be included are:</p> <ol> <li>fa: \"/path/to/species.fa\"</li> <li>blacklist: \"/path/to/blacklistbed/species.bed\"</li> </ol> <p>The following information must be included:</p> <ol> <li>regions: \"list of regions to be included; IE chr1 chr2 chr3\"</li> <li>macs2_g: \"macs2 genome shorthand; IE mm IE hs\"</li> </ol>"},{"location":"user-guide/preparing-files/#22-preparing-manifests","title":"2.2 Preparing Manifests","text":"<p>There are two manifests, one which required for all pipelines and one that is only required if running a differential analysis. These files describe information on the samples and desired contrasts. The paths of these files are defined in the snakemake_config.yaml file. These files are:</p> <ul> <li>samplemanifest</li> <li>contrasts</li> </ul>"},{"location":"user-guide/preparing-files/#221-samples-manifest-required","title":"2.2.1 Samples Manifest (REQUIRED)","text":"<p>This manifest will include information to sample level information. It includes the following column headers:</p> <ul> <li>sampleName: the sample name WITHOUT replicate number (IE \"SAMPLE\")</li> <li>replicateNumber: the sample replicate number (IE \"1\")</li> <li>isControl: whether the sample should be identified as a control (IE \"Y\")</li> <li>controlName: the name of the control to use for this sample (IE \"CONTROL\")</li> <li>controlReplicateNumber: the replicate number of the control to use for this sample (IE \"1\")</li> <li>path_to_R1: the full path to R1 fastq file (IE \"/path/to/sample1.R1.fastq\")</li> <li>path_to_R2: the full path to R1 fastq file (IE \"/path/to/sample2.R2.fastq\")</li> </ul> <p>An example sampleManifest file is shown below:</p> sampleName replicateNumber isControl controlName controlReplicateNumber path_to_R1 path_to_R2 53_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/53_H3K4me3_1.R1.fastq.gz PIPELINE_HOME/.test/53_H3K4me3_1.R2.fastq.gz 53_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/53_H3K4me3_2.R1.fastq.gz PIPELINE_HOME/.test/53_H3K4me3_2.R2.fastq.gz HN6_H3K4me3 1 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/HN6_H3K4me3_1.R1.fastq.gz PIPELINE_HOME/.test/HN6_H3K4me3_1.R2.fastq.gz HN6_H3K4me3 2 N HN6_IgG_rabbit_negative_control 1 PIPELINE_HOME/.test/HN6_H3K4me3_2.R1.fastq.gz PIPELINE_HOME/.test/HN6_H3K4me3_2.R2.fastq.gz HN6_IgG_rabbit_negative_control 1 Y - - PIPELINE_HOME/.test/HN6_IgG_rabbit_negative_control_1.R1.fastq.gz PIPELINE_HOME/.test/HN6_IgG_rabbit_negative_control_1.R2.fastq.gz"},{"location":"user-guide/run/","title":"3. Running the Pipeline","text":"<p>This should include all information about the various run commands provided within the pipeline.</p>"},{"location":"user-guide/run/#3-running-the-pipeline","title":"3. Running the Pipeline","text":""},{"location":"user-guide/run/#31-pipeline-overview","title":"3.1 Pipeline Overview","text":"<p>The Snakemake workflow has a multiple options:</p> <pre><code>Usage: bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle -m/--runmode=&lt;RUNMODE&gt; -w/--workdir=&lt;WORKDIR&gt;\n1.  RUNMODE: [Type: String] Valid options:\n    *) init : initialize workdir\n    *) run : run with slurm\n    *) reset : DELETE workdir dir and re-init it\n    *) dryrun : dry run snakemake to generate DAG\n    *) unlock : unlock workdir if locked by snakemake\n    *) runlocal : run without submitting to sbatch\n    *) testrun: run on cluster with included test dataset\n2.  WORKDIR: [Type: String]: Absolute or relative path to the output folder with write permissions.\n</code></pre>"},{"location":"user-guide/run/#32-commands-explained","title":"3.2 Commands explained","text":"<p>The following explains each of the command options:</p> <ul> <li>Preparation Commands</li> <li>init (REQUIRED): This must be performed before any Snakemake run (dry, local, cluster) can be performed. This will copy the necessary config, manifest and Snakefiles needed to run the pipeline to the provided output directory.</li> <li>dryrun (OPTIONAL): This is an optional step, to be performed before any Snakemake run (local, cluster). This will check for errors within the pipeline, and ensure that you have read/write access to the files needed to run the full pipeline.</li> <li>Processing Commands</li> <li>local: This will run the pipeline on a local node. NOTE: This should only be performed on an interactive node.</li> <li>run: This will submit a master job to the cluster, and subsequent sub-jobs as needed to complete the workflow. An email will be sent when the pipeline begins, if there are any errors, and when it completes.</li> <li>Other Commands (All optional)</li> <li>unlock: This will unlock the pipeline if an error caused it to stop in the middle of a run.</li> <li>testrun: This will run a test of the pipeline with test data</li> </ul> <p>To run any of these commands, follow the the syntax:</p> <pre><code>bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=COMMAND --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/run/#33-typical-workflow","title":"3.3 Typical Workflow","text":"<p>A typical command workflow, running on the cluster, is as follows:</p> <pre><code>bash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=init --workdir=/path/to/output/dir\n\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=dryrun --workdir=/path/to/output/dir\n\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=run --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/test-info/","title":"5. Running Test Data","text":"<p>This should walk the user through the steps of running the pipeline using test data</p>"},{"location":"user-guide/test-info/#5-pipeline-tutorial","title":"5. Pipeline Tutorial","text":"<p>Welcome to the CARLISLE Pipeline Tutorial!</p>"},{"location":"user-guide/test-info/#51-getting-started","title":"5.1 Getting Started","text":"<p>Review the information on the Getting Started for a complete overview the pipeline. The tutorial below will use test data available on NIH Biowulf HPC only. All example code will assume you are running v1.0 of the pipeline, using test data available on GitHub.</p> <p>A. Change working directory to the CARLISLE repository</p> <p>B. Initialize Pipeline</p> <pre><code>bash ./path/to/dir/carlisle --runmode=init --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/test-info/#52-about-the-test-data","title":"5.2 About the test data","text":"<p>This test data consists of sub-sampled inputs, consisting of two pairs of two replicate samples and one control. The reference to be used is hg38.</p>"},{"location":"user-guide/test-info/#53-submit-the-test-data","title":"5.3 Submit the test data","text":"<p>Test data is included in the .test directory as well as the config directory.</p> <p>A Run the test command to prepare the data, perform a dry-run and submit to the cluster</p> <pre><code>bash ./path/to/dir/carlisle --runmode=testrun --workdir=/path/to/output/dir\n</code></pre> <ul> <li>An expected output for the <code>testrun</code> is as follows:</li> </ul> <pre><code>Job stats:\njob                              count    min threads    max threads\n-----------------------------  -------  -------------  -------------\nDESeq                                  24              1              1\nalign                                   9             56             56\nalignstats                              9              2              2\nall                                     1              1              1\nbam2bg                                  9              4              4\ncreate_contrast_data_files             24              1              1\ncreate_contrast_peakcaller_files       12              1              1\ncreate_reference                        1             32             32\ncreate_replicate_sample_table           1              1              1\ndiffbb                                 24              1              1\nfilter                                 18              2              2\nfindMotif                              96              6              6\ngather_alignstats                       1              1              1\ngo_enrichment                          12              1              1\ngopeaks_broad                          16              2              2\ngopeaks_narrow                         16              2              2\nmacs2_broad                            16              2              2\nmacs2_narrow                           16              2              2\nmake_counts_matrix                     24              1              1\nmultiqc                                 2              1              1\nqc_fastqc                               9              1              1\nrose                                   96              2              2\nseacr_relaxed                          16              2              2\nseacr_stringent                        16              2              2\nspikein_assessment                      1              1              1\ntrim                                    9             56             56\ntotal                                 478              1             56\n</code></pre>"},{"location":"user-guide/test-info/#54-review-outputs","title":"5.4 Review outputs","text":"<p>Review the expected outputs on the Output page. If there are errors, review and performing stesp described on the Troubleshooting page as needed.</p>"},{"location":"user-guide/troubleshooting/","title":"Troubleshooting","text":"<p>This should include basic information on how to troubleshoot the pipeline. It should also include the main pipeliner developers contact information for users to utilize, as needed.</p>"},{"location":"user-guide/troubleshooting/#troubleshooting","title":"Troubleshooting","text":"<p>Recommended steps to troubleshoot the pipeline.</p>"},{"location":"user-guide/troubleshooting/#11-email","title":"1.1 Email","text":"<p>Check your email for an email regarding pipeline failure. You will receive an email from slurm@biowulf.nih.gov with the subject: Slurm Job_id=[#] Name=CARLISLE Failed, Run time [time], FAILED, ExitCode 1</p>"},{"location":"user-guide/troubleshooting/#12-review-the-log-files","title":"1.2 Review the log files","text":"<p>Review the logs in two ways:</p> <ol> <li>Review the master slurm file: This file will be found in the <code>/path/to/results/dir/</code> and titled <code>slurm-[jobid].out</code>. Reviewing this file will tell you what rule errored, and for any local SLURM jobs, provide error details</li> <li>Review the individual rule log files: After reviewing the master slurm-file, review the specific rules that failed within the <code>/path/to/results/dir/logs/</code>. Each rule will include a <code>.err</code> and <code>.out</code> file, with the following formatting: <code>{rulename}.{masterjobID}.{individualruleID}.{wildcards from the rule}.{out or err}</code></li> </ol>"},{"location":"user-guide/troubleshooting/#13-restart-the-run","title":"1.3 Restart the run","text":"<p>After addressing the issue, unlock the output directory, perform another dry-run and check the status of the pipeline, then resubmit to the cluster.</p> <pre><code>#unlock dir\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=unlock --workdir=/path/to/output/dir\n\n#perform dry-run\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=dryrun --workdir=/path/to/output/dir\n\n#submit to cluster\nbash ./data/CCBR_Pipeliner/Pipelines/CARLISLE/carlisle --runmode=run --workdir=/path/to/output/dir\n</code></pre>"},{"location":"user-guide/troubleshooting/#14-contact-information","title":"1.4 Contact information","text":"<p>If after troubleshooting, the error cannot be resolved, or if a bug is found, please create an issue and send and email to Samantha Chill.</p>"}]}